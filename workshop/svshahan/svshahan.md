### Experiments with different values of K:

The plots in the experiments show that lower values of k create separated boundaries. This can represent overfitting case. Too high values add a lot of bias.

### Choosing best K:

Should be done using cross-validation on a metric of choice like accuracy score, precision, recall, or f1

### Explore different kernels of Support Vector Machine:

Different kernels like rbf, linear, poly create different decision boundries for the classification

### Explian how we should choose the kernel.

Should be done using cross-validation on a metric of choice by fitting different models with different kernels and choose the best one
