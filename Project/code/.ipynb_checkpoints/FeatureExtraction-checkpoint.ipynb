{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/total_fixed.csv', parse_dates=['Changed', 'Opened'])\n",
    "cols = data.columns\n",
    "\n",
    "n_rows = len(data)\n",
    "n_cols = len(cols)\n",
    "bugIDs = data['Bug ID']\n",
    "# print data.head()\n",
    "# print bugIDs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   2001-10-10 22:21:00\n",
      "1   2001-10-10 22:25:00\n",
      "2   2001-10-10 22:39:00\n",
      "3   2001-10-10 22:40:00\n",
      "4   2001-10-10 22:40:00\n",
      "Name: Changed, dtype: datetime64[ns]\n",
      "set(['major', 'normal', 'blocker', 'critical', 'enhancement', 'trivial', 'minor'])\n"
     ]
    }
   ],
   "source": [
    "print data['Changed'].head()\n",
    "print set(data['Severity'])\n",
    "\n",
    "severity_levels = {'enhancement': 0, 'trivial' : 1, 'minor' : 2, 'normal' : 3, 'major' : 4, 'critical' : 5, 'blocker' : 6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   2001-10-10 22:21:00\n",
      "1   2001-10-10 22:25:00\n",
      "2   2001-10-10 22:39:00\n",
      "3   2001-10-10 22:40:00\n",
      "4   2001-10-10 22:40:00\n",
      "Name: Changed, dtype: datetime64[ns]\n",
      "set(['major', 'normal', 'blocker', 'critical', 'enhancement', 'trivial', 'minor'])\n"
     ]
    }
   ],
   "source": [
    "print data['Changed'].head()\n",
    "print set(data['Severity'])\n",
    "\n",
    "severity_levels = {'enhancement': 0, 'trivial' : 1, 'minor' : 2, 'normal' : 3, 'major' : 4, 'critical' : 5, 'blocker' : 6}\n",
    "np_opened = np.array(data['Opened'])\n",
    "np_opened = np_opened.reshape(len(np_opened), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generating Temporal Features\n",
    "def tmp1(x, n):\n",
    "    return sum(data['Opened'] < x) - sum (data['Opened'] < (x - timedelta(days=n)))\n",
    "\n",
    "def tmp1_np(x, n):\n",
    "    return sum(np_opened < x) - sum (np_opened < (x - np.timedelta64(n, 'D')))\n",
    "\n",
    "\n",
    "def tmp2(x, n):\n",
    "    subdata = data[data['Severity'] == x['Severity']]\n",
    "    return sum(subdata['Opened'] < x['Opened']) - sum (subdata['Opened'] < (x['Opened'] - timedelta(days=n)))\n",
    "\n",
    "def subdatagen(x, severity):\n",
    "    if severity_levels[x['Severity']] >= severity_levels[severity]:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def tmp3(i, n):\n",
    "    condn = data.apply(lambda x: subdatagen(x, i['Severity']), axis=1)\n",
    "    subdata = data[condn]\n",
    "    return sum(subdata['Opened'] < x['Opened']) - sum (subdata['Opened'] < (x['Opened'] - timedelta(days=n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2001-10-03T22:21:00.000000000'], dtype='datetime64[ns]')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data['Opened'].map(lambda x: tmp1(x, 7))\n",
    "np_opened[0] - np.timedelta64(7, 'D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([655])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp1(np_opened[0], 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "655"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np_opened < np_opened[0])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "num_partitions = 10 #number of partitions to split dataframe\n",
    "num_cores = 4 #number of cores on your machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parallelize_dataframe(df, func):\n",
    "    df_split = np.array_split(df, num_partitions)\n",
    "    pool = Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multiply_columns(data):\n",
    "    data['TMP1'] = data['Opened'].apply(lambda x: tmp1(x, 7))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data2 = parallelize_dataframe(data, multiply_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-439144ab9f21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mTMP1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_along_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtmp1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp_opened\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/iankurgarg/anaconda/lib/python2.7/site-packages/numpy/lib/shape_base.pyc\u001b[0m in \u001b[0;36mapply_along_axis\u001b[0;34m(func1d, axis, arr, *args, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m                 \u001b[0mn\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m             \u001b[0moutarr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mk\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-439144ab9f21>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mTMP1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_along_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtmp1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp_opened\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-7e81cbccecd3>\u001b[0m in \u001b[0;36mtmp1\u001b[0;34m(x, n)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Generating Temporal Features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtmp1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp_opened\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msum\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp_opened\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimedelta64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'D'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtmp2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "TMP1 = np.apply_along_axis(lambda x: tmp1(x, 7), 1, np_opened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import DateType\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)\n",
    "df = sqlContext.read.format(\"com.databricks.spark.csv\").options(header=\"true\", inferschema=\"true\", dateFormat=\"%m/%d/%y %H:%M\").load(\"../data/total_fixed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "func =  udf (lambda x: datetime.strptime(x, '%m/%d/%y %H:%M'), DateType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df.withColumn('Changed', func(col('Changed')))\n",
    "df = df.withColumn('Opened', func(col('Opened')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o669.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 26.0 failed 1 times, most recent failure: Lost task 0.0 in stage 26.0 (TID 53, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 104, in <lambda>\n    func = lambda _, it: map(mapper, it)\n  File \"<string>\", line 1, in <lambda>\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 69, in <lambda>\n    return lambda *a: toInternal(f(*a))\n  File \"<ipython-input-66-857a3e73281d>\", line 1, in <lambda>\n  File \"/Users/iankurgarg/anaconda/lib/python2.7/_strptime.py\", line 332, in _strptime\n    (data_string, format))\nValueError: time data ' \"\"protected\"\" etc. in dialogs  (1GIYQKG)\"' does not match format '%m/%d/%y %H:%M'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:144)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:87)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:336)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:2853)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2153)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2153)\n\tat org.apache.spark.sql.Dataset$$anonfun$55.apply(Dataset.scala:2837)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:2836)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2153)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2366)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:245)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 104, in <lambda>\n    func = lambda _, it: map(mapper, it)\n  File \"<string>\", line 1, in <lambda>\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 69, in <lambda>\n    return lambda *a: toInternal(f(*a))\n  File \"<ipython-input-66-857a3e73281d>\", line 1, in <lambda>\n  File \"/Users/iankurgarg/anaconda/lib/python2.7/_strptime.py\", line 332, in _strptime\n    (data_string, format))\nValueError: time data ' \"\"protected\"\" etc. in dialogs  (1GIYQKG)\"' does not match format '%m/%d/%y %H:%M'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:144)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:87)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-3106d06b6e49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.pyc\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate)\u001b[0m\n\u001b[1;32m    334\u001b[0m         \"\"\"\n\u001b[1;32m    335\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o669.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 26.0 failed 1 times, most recent failure: Lost task 0.0 in stage 26.0 (TID 53, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 104, in <lambda>\n    func = lambda _, it: map(mapper, it)\n  File \"<string>\", line 1, in <lambda>\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 69, in <lambda>\n    return lambda *a: toInternal(f(*a))\n  File \"<ipython-input-66-857a3e73281d>\", line 1, in <lambda>\n  File \"/Users/iankurgarg/anaconda/lib/python2.7/_strptime.py\", line 332, in _strptime\n    (data_string, format))\nValueError: time data ' \"\"protected\"\" etc. in dialogs  (1GIYQKG)\"' does not match format '%m/%d/%y %H:%M'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:144)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:87)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:336)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:2853)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2153)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2153)\n\tat org.apache.spark.sql.Dataset$$anonfun$55.apply(Dataset.scala:2837)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:2836)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2153)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2366)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:245)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 104, in <lambda>\n    func = lambda _, it: map(mapper, it)\n  File \"<string>\", line 1, in <lambda>\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 69, in <lambda>\n    return lambda *a: toInternal(f(*a))\n  File \"<ipython-input-66-857a3e73281d>\", line 1, in <lambda>\n  File \"/Users/iankurgarg/anaconda/lib/python2.7/_strptime.py\", line 332, in _strptime\n    (data_string, format))\nValueError: time data ' \"\"protected\"\" etc. in dialogs  (1GIYQKG)\"' does not match format '%m/%d/%y %H:%M'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:144)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:87)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can not merge type <class 'pyspark.sql.types.StringType'> and <class 'pyspark.sql.types.DoubleType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-b9c6e91061b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/total_fixed.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/context.pyc\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0mPy4JJavaError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \"\"\"\n\u001b[0;32m--> 333\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/session.pyc\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    535\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m         \u001b[0mjrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerDeUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoJavaArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java_object_rdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplySchemaToPythonRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/session.pyc\u001b[0m in \u001b[0;36m_createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m             \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferSchemaFromList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/session.pyc\u001b[0m in \u001b[0;36m_inferSchemaFromList\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    331\u001b[0m             warnings.warn(\"inferring schema from dict is deprecated,\"\n\u001b[1;32m    332\u001b[0m                           \"please use pyspark.sql.Row instead\")\n\u001b[0;32m--> 333\u001b[0;31m         \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_merge_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_infer_schema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_has_nulltype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Some of types cannot be determined after inferring\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/types.pyc\u001b[0m in \u001b[0;36m_merge_type\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m   1021\u001b[0m         \u001b[0mnfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataType\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m         fields = [StructField(f.name, _merge_type(f.dataType, nfs.get(f.name, NullType())))\n\u001b[0;32m-> 1023\u001b[0;31m                   for f in a.fields]\n\u001b[0m\u001b[1;32m   1024\u001b[0m         \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnfs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/types.pyc\u001b[0m in \u001b[0;36m_merge_type\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m   1015\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m         \u001b[0;31m# TODO: type cast (such as int -> long)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1017\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can not merge type %s and %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m     \u001b[0;31m# same type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Can not merge type <class 'pyspark.sql.types.StringType'> and <class 'pyspark.sql.types.DoubleType'>"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('../data/total_fixed.csv')\n",
    "df = sqlContext.createDataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Bug ID   Product        Component  \\\n",
      "0         1868  Platform               UI   \n",
      "1         2072  Platform               UI   \n",
      "2         2583  Platform               UI   \n",
      "3         2630  Platform               UI   \n",
      "4         2637  Platform               UI   \n",
      "5         2638  Platform               UI   \n",
      "6         2640  Platform               UI   \n",
      "7         2680  Platform               UI   \n",
      "8         2687  Platform               UI   \n",
      "9         2728  Platform               UI   \n",
      "10        3469       JDT               UI   \n",
      "11        3521       JDT               UI   \n",
      "12        3543       JDT               UI   \n",
      "13        3597       JDT               UI   \n",
      "14        4306       JDT               UI   \n",
      "15        4304       JDT               UI   \n",
      "16        4326       JDT               UI   \n",
      "17        4297       JDT               UI   \n",
      "18        4286       JDT               UI   \n",
      "19        4285       JDT               UI   \n",
      "20        4281       JDT               UI   \n",
      "21        4277       JDT               UI   \n",
      "22        4274       JDT               UI   \n",
      "23        3753       JDT               UI   \n",
      "24        3763       JDT               UI   \n",
      "25        4264       JDT               UI   \n",
      "26        3756       JDT               UI   \n",
      "27        4087       JDT               UI   \n",
      "28        4256       JDT               UI   \n",
      "29        4246       JDT               UI   \n",
      "...        ...       ...              ...   \n",
      "103775  207744  Platform              SWT   \n",
      "103776  212943       JDT            Debug   \n",
      "103777  199642   Equinox        Incubator   \n",
      "103778  210593   Equinox        Incubator   \n",
      "103779  212201       JDT               UI   \n",
      "103780  212035       JDT               UI   \n",
      "103781  211791       JDT               UI   \n",
      "103782  210940       JDT               UI   \n",
      "103783  212221       JDT               UI   \n",
      "103784  109854  Platform             Text   \n",
      "103785  209466  Platform               UI   \n",
      "103786  210535  Platform               UI   \n",
      "103787  212914       JDT               UI   \n",
      "103788  212913  Platform               UI   \n",
      "103789  212997  Platform              SWT   \n",
      "103790  213016       JDT            Debug   \n",
      "103791  212048   Equinox        Framework   \n",
      "103792  211904   Equinox        Framework   \n",
      "103793  212854  Platform  User Assistance   \n",
      "103794   38560       JDT            Debug   \n",
      "103795   37078       PDE               UI   \n",
      "103796   48656  Platform            Debug   \n",
      "103797  213035  Platform               UI   \n",
      "103798  212950  Platform        Resources   \n",
      "103799  213009  Platform              SWT   \n",
      "103800  207029  Platform               UI   \n",
      "103801  212633   Equinox        Incubator   \n",
      "103802  213062  Platform               UI   \n",
      "103803  205866  Platform              SWT   \n",
      "103804  213066  Platform              SWT   \n",
      "\n",
      "                                    Assignee    Status  Resolution  \\\n",
      "0                   Kevin_Haaland@ca.ibm.com    CLOSED       FIXED   \n",
      "1                   Kevin_Haaland@ca.ibm.com    CLOSED       FIXED   \n",
      "2                   Kevin_Haaland@ca.ibm.com    CLOSED       FIXED   \n",
      "3                   Kevin_Haaland@ca.ibm.com    CLOSED       FIXED   \n",
      "4                   Kevin_Haaland@ca.ibm.com    CLOSED       FIXED   \n",
      "5                   Kevin_Haaland@ca.ibm.com    CLOSED       FIXED   \n",
      "6                   Kevin_Haaland@ca.ibm.com    CLOSED       FIXED   \n",
      "7                   Kevin_Haaland@ca.ibm.com    CLOSED       FIXED   \n",
      "8                   Kevin_Haaland@ca.ibm.com    CLOSED       FIXED   \n",
      "9                   Kevin_Haaland@ca.ibm.com    CLOSED       FIXED   \n",
      "10                  andre_weinand@ch.ibm.com  RESOLVED     WONTFIX   \n",
      "11                    erich_gamma@ch.ibm.com  VERIFIED       FIXED   \n",
      "12                    erich_gamma@ch.ibm.com  VERIFIED       FIXED   \n",
      "13                    erich_gamma@ch.ibm.com  VERIFIED       FIXED   \n",
      "14                    erich_gamma@ch.ibm.com  VERIFIED       FIXED   \n",
      "15                    erich_gamma@ch.ibm.com  VERIFIED       FIXED   \n",
      "16                kai-uwe_maetzel@ch.ibm.com  RESOLVED       FIXED   \n",
      "17                    erich_gamma@ch.ibm.com  VERIFIED       FIXED   \n",
      "18                    erich_gamma@ch.ibm.com  VERIFIED       FIXED   \n",
      "19                    erich_gamma@ch.ibm.com  VERIFIED       FIXED   \n",
      "20                    erich_gamma@ch.ibm.com  VERIFIED       FIXED   \n",
      "21                    erich_gamma@ch.ibm.com  VERIFIED       FIXED   \n",
      "22                    erich_gamma@ch.ibm.com  VERIFIED       FIXED   \n",
      "23                    erich_gamma@ch.ibm.com  VERIFIED       FIXED   \n",
      "24                    erich_gamma@ch.ibm.com  VERIFIED       FIXED   \n",
      "25                kai-uwe_maetzel@ch.ibm.com  RESOLVED       FIXED   \n",
      "26                kai-uwe_maetzel@ch.ibm.com  RESOLVED     INVALID   \n",
      "27                    erich_gamma@ch.ibm.com  VERIFIED       FIXED   \n",
      "28                    erich_gamma@ch.ibm.com  VERIFIED       FIXED   \n",
      "29                    erich_gamma@ch.ibm.com  VERIFIED       FIXED   \n",
      "...                                      ...       ...         ...   \n",
      "103775                   snorthov@ca.ibm.com  VERIFIED       FIXED   \n",
      "103776           jdt-debug-inbox@eclipse.org       NEW         ---   \n",
      "103777   equinox.incubator-inbox@eclipse.org  RESOLVED     WONTFIX   \n",
      "103778   equinox.incubator-inbox@eclipse.org  RESOLVED   DUPLICATE   \n",
      "103779              jdt-ui-inbox@eclipse.org  RESOLVED       FIXED   \n",
      "103780              jdt-ui-inbox@eclipse.org  RESOLVED       FIXED   \n",
      "103781              jdt-ui-inbox@eclipse.org  RESOLVED       FIXED   \n",
      "103782              jdt-ui-inbox@eclipse.org  RESOLVED       FIXED   \n",
      "103783              jdt-ui-inbox@eclipse.org  RESOLVED  WORKSFORME   \n",
      "103784       platform-text-inbox@eclipse.org  RESOLVED  WORKSFORME   \n",
      "103785                Tod_Creasey@ca.ibm.com  VERIFIED       FIXED   \n",
      "103786                 eclipse@pookzilla.net  VERIFIED       FIXED   \n",
      "103787              jdt-ui-inbox@eclipse.org  RESOLVED     INVALID   \n",
      "103788         Platform-UI-Inbox@eclipse.org  VERIFIED       FIXED   \n",
      "103789        platform-swt-inbox@eclipse.org  RESOLVED   DUPLICATE   \n",
      "103790           jdt-debug-inbox@eclipse.org  RESOLVED   DUPLICATE   \n",
      "103791   equinox.framework-inbox@eclipse.org  RESOLVED       FIXED   \n",
      "103792   equinox.framework-inbox@eclipse.org  RESOLVED       FIXED   \n",
      "103793         platform-ua-inbox@eclipse.org       NEW         ---   \n",
      "103794           jdt-debug-inbox@eclipse.org  RESOLVED  WORKSFORME   \n",
      "103795              pde-ui-inbox@eclipse.org  RESOLVED   DUPLICATE   \n",
      "103796      platform-debug-inbox@eclipse.org  RESOLVED     WONTFIX   \n",
      "103797       platform-ui-triaged@eclipse.org       NEW         ---   \n",
      "103798  platform-resources-inbox@eclipse.org  RESOLVED     INVALID   \n",
      "103799        platform-swt-inbox@eclipse.org  RESOLVED   DUPLICATE   \n",
      "103800                   timothym@ca.ibm.com  VERIFIED       FIXED   \n",
      "103801                dj.houghton@ca.ibm.com  RESOLVED       FIXED   \n",
      "103802                   bokowski@google.com       NEW         ---   \n",
      "103803                  cocoakevin@gmail.com  RESOLVED       FIXED   \n",
      "103804        platform-swt-inbox@eclipse.org  RESOLVED   DUPLICATE   \n",
      "\n",
      "                                                  Summary         Changed  \\\n",
      "0                              Task Smoke Tests (1GHE30L)  10/10/01 22:21   \n",
      "1                       Preferences Smoke Tests (1GIIP94)  10/10/01 22:25   \n",
      "2                         Navigator Smoke Tests (1GHDUB4)  10/10/01 22:39   \n",
      "3                           Outline Smoke Tests (1GHFYSV)  10/10/01 22:40   \n",
      "4                   Properties View Smoke Tests (1GHFYQ0)  10/10/01 22:40   \n",
      "5                    Welcome Editor Smoke Tests (1GHFZ5B)  10/10/01 22:40   \n",
      "6                       Text Editor Smoke Tests (1GHFZ95)  10/10/01 22:40   \n",
      "7                     Import Wizard Smoke Tests (1GI3ZW4)  10/10/01 22:41   \n",
      "8                     Export Wizard Smoke Tests (1GI5JX0)  10/10/01 22:41   \n",
      "9                 Properties Dialog Smoke Tests (1GIIPDQ)  10/10/01 22:42   \n",
      "10                Walkback in classloader on J9 (1GIEN3W)   10/11/01 5:35   \n",
      "11      type hiearchy: need a short cut to toggle from...   10/11/01 5:43   \n",
      "12      Can select folder outside project as bin outpu...   10/11/01 5:55   \n",
      "13                        No editor preferences (1GBPMBT)   10/11/01 6:00   \n",
      "14         internal error in refactoring wizard (1GL4LRF)   10/11/01 6:06   \n",
      "15      JAR Packager dialog to small if opened directl...   10/11/01 6:20   \n",
      "16      SWTException from CompletionProposalPopup (1GL...   10/11/01 6:31   \n",
      "17                   Deletion of packages fails (1GKZK3V)   10/11/01 6:37   \n",
      "18      organize imports - 'not parsed' error should b...   10/11/01 6:44   \n",
      "19      term 'parsed' is inappropriate in the ui (1GKZ...   10/11/01 6:48   \n",
      "20                        cannot move resources (1GKXCTH)   10/11/01 6:51   \n",
      "21            Need better text in error message (1GKX9M5)   10/11/01 6:54   \n",
      "22      Search: stepping through search matches requir...   10/11/01 7:07   \n",
      "23      Source lookup fails in self-host even for simp...   10/11/01 7:09   \n",
      "24      Able to add duplicate method entry breakpoints...   10/11/01 8:12   \n",
      "25                     SWT Error in Java editor (1GKX6ZZ)   10/11/01 8:12   \n",
      "26      Smoke 117: Editor doesnt' show what is compile...   10/11/01 8:24   \n",
      "27      Compiler option settings not properly persiste...   10/11/01 8:27   \n",
      "28                        JRE vs. Installed JRE (1GKM61G)   10/11/01 8:31   \n",
      "29        Refactoring: Progress messages format (1GKEXYL)   10/11/01 8:41   \n",
      "...                                                   ...             ...   \n",
      "103775  [Contributions] Debug toolbar dropdown menu cl...  12/13/07 16:28   \n",
      "103776  Variables with large String values hang the wo...  12/13/07 16:41   \n",
      "103777  [prov] Use 'bundles' instead of 'plugins' for ...  12/13/07 17:05   \n",
      "103778         [prov] cannot start after M2 -> M3 upgrade  12/13/07 17:23   \n",
      "103779               [JUnit] NPE trying to run some tests   12/14/07 6:18   \n",
      "103780         [search] UI for finegrained search options   12/14/07 6:24   \n",
      "103781         [JUnit] JUnit launch shortcut enhancements   12/14/07 6:24   \n",
      "103782                      [open type] Open type is slow   12/14/07 6:25   \n",
      "103783  [build path] Create project from existing sour...   12/14/07 6:47   \n",
      "103784  [rulers] Wrong location of last mouse button a...   12/14/07 7:53   \n",
      "103785  [Markers] Cannot select working set in Show Ma...   12/14/07 8:02   \n",
      "103786  [WorkbenchLauncher] build id doesn't line up w...   12/14/07 8:21   \n",
      "103787          protected interface is marked as an error   12/14/07 9:11   \n",
      "103788                        perspective bar not resized   12/14/07 9:34   \n",
      "103789  No more Error Reporting when binding in SWT-pr...   12/14/07 9:37   \n",
      "103790           Allow to set interface method breakpoint   12/14/07 9:48   \n",
      "103791  [launcher] Running from outside current direct...  12/14/07 11:11   \n",
      "103792  [launcher] Running from outside current direct...  12/14/07 11:12   \n",
      "103793      [Forms] FormEditor: Lack of API documentation  12/14/07 11:23   \n",
      "103794  [scrapbook] Scrapbook: Debug Perspective opene...  12/14/07 11:28   \n",
      "103795          add the ability to create source features  12/14/07 11:30   \n",
      "103796  [string variables] Variable expansion written ...  12/14/07 11:41   \n",
      "103797  [Commands] ContentAssistCommandAdapter without...  12/14/07 11:41   \n",
      "103798  Preferences from project deleted w/o contents ...  12/14/07 11:44   \n",
      "103799  eclipse crashes on mac os 10.5 (leopard) when ...  12/14/07 11:47   \n",
      "103800  [Commands] NullPointerException in HandlerActi...  12/14/07 12:00   \n",
      "103801  [prov] Exception when trying to run an upgrade...  12/14/07 12:04   \n",
      "103802  Batch calls into threading realms to minimize ...  12/14/07 15:36   \n",
      "103803            SWT.Settings - no notification on Vista  12/14/07 16:13   \n",
      "103804  OSX SWT - Crash on running org.eclipse.draw2d....  12/14/07 16:55   \n",
      "\n",
      "       Priority     Severity  Keywords  \\\n",
      "0            P3       normal      test   \n",
      "1            P3       normal      test   \n",
      "2            P3       normal      test   \n",
      "3            P3       normal      test   \n",
      "4            P3       normal      test   \n",
      "5            P3       normal      test   \n",
      "6            P3       normal      test   \n",
      "7            P3       normal      test   \n",
      "8            P3       normal      test   \n",
      "9            P3       normal      test   \n",
      "10           P3       normal       NaN   \n",
      "11           P3        minor       NaN   \n",
      "12           P3       normal       NaN   \n",
      "13           P3     critical       NaN   \n",
      "14           P3       normal       NaN   \n",
      "15           P3       normal       NaN   \n",
      "16           P1       normal       NaN   \n",
      "17           P3       normal       NaN   \n",
      "18           P3       normal       NaN   \n",
      "19           P3       normal       NaN   \n",
      "20           P3       normal       NaN   \n",
      "21           P3       normal       NaN   \n",
      "22           P3       normal       NaN   \n",
      "23           P1     critical       NaN   \n",
      "24           P3        major       NaN   \n",
      "25           P1       normal       NaN   \n",
      "26           P1     critical       NaN   \n",
      "27           P3       normal       NaN   \n",
      "28           P3       normal       NaN   \n",
      "29           P3       normal       NaN   \n",
      "...         ...          ...       ...   \n",
      "103775       P3        major       NaN   \n",
      "103776       P3        major       NaN   \n",
      "103777       P3       normal       NaN   \n",
      "103778       P3       normal       NaN   \n",
      "103779       P3       normal       NaN   \n",
      "103780       P3       normal       NaN   \n",
      "103781       P3       normal       NaN   \n",
      "103782       P3       normal       NaN   \n",
      "103783       P3  enhancement       NaN   \n",
      "103784       P3       normal       NaN   \n",
      "103785       P3        major       NaN   \n",
      "103786       P3       normal       NaN   \n",
      "103787       P3       normal       NaN   \n",
      "103788       P3       normal       NaN   \n",
      "103789       P3     critical       NaN   \n",
      "103790       P3  enhancement       NaN   \n",
      "103791       P3       normal       NaN   \n",
      "103792       P3       normal       NaN   \n",
      "103793       P3       normal       NaN   \n",
      "103794       P3       normal       NaN   \n",
      "103795       P3  enhancement       NaN   \n",
      "103796       P3        minor       NaN   \n",
      "103797       P3       normal       NaN   \n",
      "103798       P3       normal       NaN   \n",
      "103799       P3        major  needinfo   \n",
      "103800       P3      trivial       NaN   \n",
      "103801       P3       normal       NaN   \n",
      "103802       P3       normal       NaN   \n",
      "103803       P3       normal       NaN   \n",
      "103804       P3        major       NaN   \n",
      "\n",
      "                                                Summary.1          Opened  \n",
      "0                              Task Smoke Tests (1GHE30L)  10/10/01 22:21  \n",
      "1                       Preferences Smoke Tests (1GIIP94)  10/10/01 22:25  \n",
      "2                         Navigator Smoke Tests (1GHDUB4)  10/10/01 22:39  \n",
      "3                           Outline Smoke Tests (1GHFYSV)  10/10/01 22:40  \n",
      "4                   Properties View Smoke Tests (1GHFYQ0)  10/10/01 22:40  \n",
      "5                    Welcome Editor Smoke Tests (1GHFZ5B)  10/10/01 22:40  \n",
      "6                       Text Editor Smoke Tests (1GHFZ95)  10/10/01 22:40  \n",
      "7                     Import Wizard Smoke Tests (1GI3ZW4)  10/10/01 22:41  \n",
      "8                     Export Wizard Smoke Tests (1GI5JX0)  10/10/01 22:41  \n",
      "9                 Properties Dialog Smoke Tests (1GIIPDQ)  10/10/01 22:42  \n",
      "10                Walkback in classloader on J9 (1GIEN3W)  10/10/01 22:55  \n",
      "11      type hiearchy: need a short cut to toggle from...  10/10/01 22:56  \n",
      "12      Can select folder outside project as bin outpu...  10/10/01 22:56  \n",
      "13                        No editor preferences (1GBPMBT)  10/10/01 22:57  \n",
      "14         internal error in refactoring wizard (1GL4LRF)  10/10/01 23:09  \n",
      "15      JAR Packager dialog to small if opened directl...  10/10/01 23:09  \n",
      "16      SWTException from CompletionProposalPopup (1GL...  10/10/01 23:09  \n",
      "17                   Deletion of packages fails (1GKZK3V)  10/10/01 23:09  \n",
      "18      organize imports - 'not parsed' error should b...  10/10/01 23:09  \n",
      "19      term 'parsed' is inappropriate in the ui (1GKZ...  10/10/01 23:09  \n",
      "20                        cannot move resources (1GKXCTH)  10/10/01 23:09  \n",
      "21            Need better text in error message (1GKX9M5)  10/10/01 23:09  \n",
      "22      Search: stepping through search matches requir...  10/10/01 23:08  \n",
      "23      Source lookup fails in self-host even for simp...  10/10/01 23:00  \n",
      "24      Able to add duplicate method entry breakpoints...  10/10/01 23:00  \n",
      "25                     SWT Error in Java editor (1GKX6ZZ)  10/10/01 23:08  \n",
      "26      Smoke 117: Editor doesnt' show what is compile...  10/10/01 23:00  \n",
      "27      Compiler option settings not properly persiste...  10/10/01 23:05  \n",
      "28                        JRE vs. Installed JRE (1GKM61G)  10/10/01 23:08  \n",
      "29        Refactoring: Progress messages format (1GKEXYL)  10/10/01 23:08  \n",
      "...                                                   ...             ...  \n",
      "103775  [Contributions] Debug toolbar dropdown menu cl...   10/29/07 5:56  \n",
      "103776  Variables with large String values hang the wo...  12/13/07 16:40  \n",
      "103777  [prov] Use 'bundles' instead of 'plugins' for ...    8/11/07 4:24  \n",
      "103778         [prov] cannot start after M2 -> M3 upgrade  11/21/07 16:38  \n",
      "103779               [JUnit] NPE trying to run some tests   12/6/07 16:26  \n",
      "103780         [search] UI for finegrained search options   12/5/07 12:31  \n",
      "103781         [JUnit] JUnit launch shortcut enhancements   12/3/07 12:40  \n",
      "103782                      [open type] Open type is slow  11/26/07 10:42  \n",
      "103783  [build path] Create project from existing sour...   12/6/07 19:27  \n",
      "103784  [rulers] Wrong location of last mouse button a...    9/19/05 5:26  \n",
      "103785  [Markers] Cannot select working set in Show Ma...   11/12/07 5:08  \n",
      "103786  [WorkbenchLauncher] build id doesn't line up w...  11/21/07 10:09  \n",
      "103787          protected interface is marked as an error  12/13/07 13:49  \n",
      "103788                        perspective bar not resized  12/13/07 13:44  \n",
      "103789  No more Error Reporting when binding in SWT-pr...   12/14/07 5:19  \n",
      "103790           Allow to set interface method breakpoint   12/14/07 9:25  \n",
      "103791  [launcher] Running from outside current direct...   12/5/07 13:09  \n",
      "103792  [launcher] Running from outside current direct...   12/4/07 13:47  \n",
      "103793      [Forms] FormEditor: Lack of API documentation   12/13/07 5:01  \n",
      "103794  [scrapbook] Scrapbook: Debug Perspective opene...     6/6/03 4:54  \n",
      "103795          add the ability to create source features   4/29/03 19:38  \n",
      "103796  [string variables] Variable expansion written ...  12/12/03 14:06  \n",
      "103797  [Commands] ContentAssistCommandAdapter without...  12/14/07 10:51  \n",
      "103798  Preferences from project deleted w/o contents ...  12/13/07 17:46  \n",
      "103799  eclipse crashes on mac os 10.5 (leopard) when ...   12/14/07 8:08  \n",
      "103800  [Commands] NullPointerException in HandlerActi...   10/22/07 4:27  \n",
      "103801  [prov] Exception when trying to run an upgrade...  12/11/07 13:16  \n",
      "103802  Batch calls into threading realms to minimize ...  12/14/07 15:36  \n",
      "103803            SWT.Settings - no notification on Vista   10/9/07 16:09  \n",
      "103804  OSX SWT - Crash on running org.eclipse.draw2d....  12/14/07 16:23  \n",
      "\n",
      "[103805 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "print (data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
